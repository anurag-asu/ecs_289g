{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ade0dcf-256d-4af6-bcae-5a1876825ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b14cab0-31e4-4e91-9371-ae5eb5369aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.8.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3454a497-787c-4ade-bd0e-2e5e8b9e9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('train_events.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "bad_words = ['Story Start', 'Story End']\n",
    "filtered_data = []\n",
    "\n",
    "for line in data:\n",
    "    if not any(bad_word in line for bad_word in bad_words):\n",
    "        filtered_data.append(line)\n",
    "        \n",
    "data = open('train_events_1.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "for line in data:\n",
    "    if not any(bad_word in line for bad_word in bad_words):\n",
    "        filtered_data.append(line)\n",
    "    \n",
    "\n",
    "filtered_data = ''.join(filtered_data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a60b71-29cc-4a83-9373-4299a03741cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10877\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(word_tokenize(filtered_data)))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eae49a5-2054-4bb1-be4f-690d15129cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736a69e1-69f4-49ea-a4bf-7dbec7cba1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 16:08:15.464361: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-13 16:08:15.464600: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ids_from_words = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "words_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5e0fd7-51e6-4e04-8219-2af91f177434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10878"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_from_words.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12109ad-280a-45bd-9377-c46bb7e39f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(input_sequence, seq_no):\n",
    "    print(len(input_sequence))\n",
    "    for index in range(len(input_sequence)):\n",
    "        if index%100 == 0:\n",
    "            print('processed', index)\n",
    "        input_sequence[index] = ids_from_words(input_sequence[index])\n",
    "    return input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f8450b-f2f1-4bea-88ca-03e948a8066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(input_sequences):\n",
    "    X = input_sequences[:,:-1]\n",
    "    Y = tf.keras.utils.to_categorical(input_sequences[:,-1], num_classes=len(ids_from_words.get_vocabulary()))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cae845-d222-4fea-8244-e1b81257822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_sequence(input_sequences, max_sequence_len):\n",
    "    print('padding')\n",
    "    return tf.convert_to_tensor(pad_sequences(input_sequences, maxlen=max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f43f941-cfd1-4510-97ac-438cad50971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "def get_dataset(X, Y):\n",
    "    dataset =  tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "    dataset = (dataset\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=False)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e14aba-7c21-4fdd-adde-05148cbcfd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event2Event(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstmunits = tf.keras.layers.LSTM(rnn_units,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.lstmunits.get_initial_state(x)\n",
    "        \n",
    "        x, memory_state, carry_state = self.lstmunits(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, memory_state, carry_state\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aee44e0-13cd-4684-8682-fc7e1df7a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(ids_from_words.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496c704e-8635-4852-867d-68817a40d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Event2Event(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_words.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf9e2bb-51f0-46cc-a1e5-f550ffbe87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"trained_events_model_second_batch_validation_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='min',\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b4f8452-ce72-41bc-84d0-a1e47b63cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "my_file = Path('saved_sequences/events.npy')\n",
    "\n",
    "count = 0\n",
    "max_count = 10000\n",
    "\n",
    "story_text = list()\n",
    "input_sequences = list()\n",
    "\n",
    "if not my_file.is_file():\n",
    "    with open(\"train_events.txt\", 'rb') as data:\n",
    "        for line in data:\n",
    "            line = line.decode(encoding='utf-8').lower().split('\\n')[0]\n",
    "            if line == 'story start':\n",
    "                story_text = list()\n",
    "                continue\n",
    "\n",
    "            if line == 'story end':\n",
    "                story_text = ''.join(story_text)\n",
    "                token_list = list(filter(None, story_text.split(' ')))\n",
    "\n",
    "                for i in range(1, len(token_list)):        \n",
    "                    n_gram_sequence = token_list[:i+1]        \n",
    "                    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "                story_text = list()\n",
    "\n",
    "            story_text.append(line)\n",
    "\n",
    "            count += 1\n",
    "            if(count == max_count):\n",
    "                print('size of sequences', len(input_sequences))\n",
    "                input_sequences = get_ids(input_sequences, 1)\n",
    "\n",
    "                max_sequence_len = max([len(x) for x in input_sequences])\n",
    "                input_sequences = pad_input_sequence(input_sequences, max_sequence_len)\n",
    "\n",
    "                with open('saved_sequences/events_1.npy', 'ab') as f:\n",
    "                    np.save(f, input_sequences.numpy())\n",
    "\n",
    "                input_sequences = list()\n",
    "                count = 0\n",
    "\n",
    "        if(len(input_sequences) > 0):\n",
    "            input_sequences = get_ids(input_sequences, 1)\n",
    "\n",
    "            max_sequence_len = max([len(x) for x in input_sequences])\n",
    "            input_sequences = pad_input_sequence(input_sequences, max_sequence_len)\n",
    "\n",
    "            with open('saved_sequences/events_1.npy', 'ab') as f:\n",
    "                    np.save(f, input_sequences.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce1b84-79c1-42e5-949c-a93842059c83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20026\n",
      "processed 0\n",
      "processed 100\n",
      "processed 200\n",
      "processed 300\n",
      "processed 400\n",
      "processed 500\n",
      "processed 600\n",
      "processed 700\n",
      "processed 800\n",
      "processed 900\n",
      "processed 1000\n",
      "processed 1100\n",
      "processed 1200\n",
      "processed 1300\n",
      "processed 1400\n",
      "processed 1500\n",
      "processed 1600\n",
      "processed 1700\n",
      "processed 1800\n",
      "processed 1900\n",
      "processed 2000\n",
      "processed 2100\n",
      "processed 2200\n",
      "processed 2300\n",
      "processed 2400\n",
      "processed 2500\n",
      "processed 2600\n",
      "processed 2700\n",
      "processed 2800\n",
      "processed 2900\n",
      "processed 3000\n",
      "processed 3100\n",
      "processed 3200\n",
      "processed 3300\n",
      "processed 3400\n",
      "processed 3500\n",
      "processed 3600\n",
      "processed 3700\n",
      "processed 3800\n",
      "processed 3900\n",
      "processed 4000\n",
      "processed 4100\n",
      "processed 4200\n",
      "processed 4300\n",
      "processed 4400\n",
      "processed 4500\n",
      "processed 4600\n",
      "processed 4700\n",
      "processed 4800\n",
      "processed 4900\n",
      "processed 5000\n",
      "processed 5100\n",
      "processed 5200\n",
      "processed 5300\n",
      "processed 5400\n",
      "processed 5500\n",
      "processed 5600\n",
      "processed 5700\n",
      "processed 5800\n",
      "processed 5900\n",
      "processed 6000\n",
      "processed 6100\n",
      "processed 6200\n",
      "processed 6300\n",
      "processed 6400\n",
      "processed 6500\n",
      "processed 6600\n",
      "processed 6700\n",
      "processed 6800\n",
      "processed 6900\n",
      "processed 7000\n",
      "processed 7100\n",
      "processed 7200\n",
      "processed 7300\n",
      "processed 7400\n",
      "processed 7500\n",
      "processed 7600\n",
      "processed 7700\n",
      "processed 7800\n",
      "processed 7900\n",
      "processed 8000\n",
      "processed 8100\n",
      "processed 8200\n",
      "processed 8300\n",
      "processed 8400\n",
      "processed 8500\n",
      "processed 8600\n",
      "processed 8700\n",
      "processed 8800\n",
      "processed 8900\n",
      "processed 9000\n",
      "processed 9100\n",
      "processed 9200\n",
      "processed 9300\n",
      "processed 9400\n",
      "processed 9500\n",
      "processed 9600\n",
      "processed 9700\n",
      "processed 9800\n",
      "processed 9900\n",
      "processed 10000\n",
      "processed 10100\n",
      "processed 10200\n",
      "processed 10300\n",
      "processed 10400\n",
      "processed 10500\n",
      "processed 10600\n",
      "processed 10700\n",
      "processed 10800\n",
      "processed 10900\n",
      "processed 11000\n",
      "processed 11100\n",
      "processed 11200\n",
      "processed 11300\n",
      "processed 11400\n",
      "processed 11500\n",
      "processed 11600\n",
      "processed 11700\n",
      "processed 11800\n",
      "processed 11900\n",
      "processed 12000\n",
      "processed 12100\n",
      "processed 12200\n",
      "processed 12300\n",
      "processed 12400\n",
      "processed 12500\n",
      "processed 12600\n",
      "processed 12700\n",
      "processed 12800\n",
      "processed 12900\n",
      "processed 13000\n",
      "processed 13100\n",
      "processed 13200\n",
      "processed 13300\n",
      "processed 13400\n",
      "processed 13500\n",
      "processed 13600\n",
      "processed 13700\n",
      "processed 13800\n",
      "processed 13900\n",
      "processed 14000\n",
      "processed 14100\n",
      "processed 14200\n",
      "processed 14300\n",
      "processed 14400\n",
      "processed 14500\n",
      "processed 14600\n",
      "processed 14700\n",
      "processed 14800\n",
      "processed 14900\n",
      "processed 15000\n",
      "processed 15100\n",
      "processed 15200\n",
      "processed 15300\n",
      "processed 15400\n",
      "processed 15500\n",
      "processed 15600\n",
      "processed 15700\n",
      "processed 15800\n",
      "processed 15900\n",
      "processed 16000\n",
      "processed 16100\n",
      "processed 16200\n",
      "processed 16300\n",
      "processed 16400\n",
      "processed 16500\n",
      "processed 16600\n",
      "processed 16700\n",
      "processed 16800\n",
      "processed 16900\n",
      "processed 17000\n",
      "processed 17100\n",
      "processed 17200\n",
      "processed 17300\n",
      "processed 17400\n",
      "processed 17500\n",
      "processed 17600\n",
      "processed 17700\n",
      "processed 17800\n",
      "processed 17900\n",
      "processed 18000\n",
      "processed 18100\n",
      "processed 18200\n",
      "processed 18300\n",
      "processed 18400\n",
      "processed 18500\n",
      "processed 18600\n",
      "processed 18700\n",
      "processed 18800\n",
      "processed 18900\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "my_file = Path('saved_sequences/test_events.npy')\n",
    "\n",
    "count = 0\n",
    "max_count = 10000\n",
    "\n",
    "story_text = list()\n",
    "input_sequences = list()\n",
    "\n",
    "if not my_file.is_file():\n",
    "    with open(\"train_events_1.txt\", 'rb') as data:\n",
    "        for line in data:\n",
    "            line = line.decode(encoding='utf-8').lower().split('\\n')[0]\n",
    "            if line == 'story start':\n",
    "                story_text = list()\n",
    "                continue\n",
    "\n",
    "            if line == 'story end':\n",
    "                story_text = ''.join(story_text)\n",
    "                token_list = list(filter(None, story_text.split(' ')))\n",
    "\n",
    "                for i in range(1, len(token_list)):        \n",
    "                    n_gram_sequence = token_list[:i+1]        \n",
    "                    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "                story_text = list()\n",
    "\n",
    "            story_text.append(line)\n",
    "\n",
    "            count += 1\n",
    "            if(count == max_count):\n",
    "                print('size of sequences', len(input_sequences))\n",
    "                input_sequences = get_ids(input_sequences, 1)\n",
    "\n",
    "                max_sequence_len = max([len(x) for x in input_sequences])\n",
    "                input_sequences = pad_input_sequence(input_sequences, max_sequence_len)\n",
    "\n",
    "                with open('saved_sequences/test_events.npy', 'ab') as f:\n",
    "                    np.save(f, input_sequences.numpy())\n",
    "\n",
    "                input_sequences = list()\n",
    "                count = 0\n",
    "\n",
    "        if(len(input_sequences) > 0):\n",
    "            input_sequences = get_ids(input_sequences, 1)\n",
    "\n",
    "            max_sequence_len = max([len(x) for x in input_sequences])\n",
    "            input_sequences = pad_input_sequence(input_sequences, max_sequence_len)\n",
    "\n",
    "            with open('saved_sequences/test_events.npy', 'ab') as f:\n",
    "                    np.save(f, input_sequences.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "603e6058-490d-4bfc-a98a-0fa3c8e0f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_sequences/events.npy', 'rb') as f:\n",
    "    fsz = os.fstat(f.fileno()).st_size\n",
    "    input_sequences = np.load(f)\n",
    "    while f.tell() < fsz:\n",
    "        input_sequences = np.vstack((input_sequences, np.load(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f87468-f0a6-455f-b65f-f3a2de68f50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200127"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf955e5-5a5e-46c1-a49b-7b069758881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process input sequences in batches of 50000 out of 200127to save application from crashing\n",
    "input_sequences = input_sequences[150000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "985d932e-cf90-478a-a7ac-a9c0a8ccf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04554e83-5f96-4c37-8132-1f8669a7adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.join(words_from_ids(ids), separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548926ae-0fc8-4789-8e56-40aab38bbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3150b394-3dcb-42f7-b2b2-b055a9b25567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15868bac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0860d4aa-799b-47df-b52b-4a0caa659f34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 22:50:23.877316: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-11 22:50:24.007473: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/49 [..............................] - ETA: 1:02 - loss: 3.2098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 22:50:24.147247: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 0s - loss: 2.7501\n",
      "Epoch 1: val_loss improved from inf to 3.25502, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 19s 374ms/step - loss: 2.7501 - val_loss: 3.2550\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6769\n",
      "Epoch 1: val_loss improved from 3.25502 to 3.07750, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 18s 373ms/step - loss: 2.6769 - val_loss: 3.0775\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6741\n",
      "Epoch 1: val_loss improved from 3.07750 to 2.83741, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 18s 378ms/step - loss: 2.6741 - val_loss: 2.8374\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.8131\n",
      "Epoch 1: val_loss did not improve from 2.83741\n",
      "49/49 [==============================] - 18s 371ms/step - loss: 2.8131 - val_loss: 2.9580\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0644\n",
      "Epoch 1: val_loss did not improve from 2.83741\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.0644 - val_loss: 2.9613\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6539\n",
      "Epoch 1: val_loss did not improve from 2.83741\n",
      "49/49 [==============================] - 18s 371ms/step - loss: 2.6539 - val_loss: 3.1904\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6443\n",
      "Epoch 1: val_loss did not improve from 2.83741\n",
      "49/49 [==============================] - 18s 377ms/step - loss: 2.6443 - val_loss: 3.0117\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6529\n",
      "Epoch 1: val_loss improved from 2.83741 to 2.80749, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 18s 380ms/step - loss: 2.6529 - val_loss: 2.8075\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.7598\n",
      "Epoch 1: val_loss did not improve from 2.80749\n",
      "49/49 [==============================] - 18s 367ms/step - loss: 2.7598 - val_loss: 2.9386\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0033\n",
      "Epoch 1: val_loss did not improve from 2.80749\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.0033 - val_loss: 2.9421\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6247\n",
      "Epoch 1: val_loss did not improve from 2.80749\n",
      "49/49 [==============================] - 18s 376ms/step - loss: 2.6247 - val_loss: 3.2347\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5956\n",
      "Epoch 1: val_loss did not improve from 2.80749\n",
      "49/49 [==============================] - 18s 375ms/step - loss: 2.5956 - val_loss: 3.0343\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5980\n",
      "Epoch 1: val_loss improved from 2.80749 to 2.78314, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 19s 391ms/step - loss: 2.5980 - val_loss: 2.7831\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6978\n",
      "Epoch 1: val_loss did not improve from 2.78314\n",
      "49/49 [==============================] - 18s 377ms/step - loss: 2.6978 - val_loss: 2.9742\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9471\n",
      "Epoch 1: val_loss did not improve from 2.78314\n",
      "1/1 [==============================] - 9s 9s/step - loss: 1.9471 - val_loss: 2.9700\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5940\n",
      "Epoch 1: val_loss did not improve from 2.78314\n",
      "49/49 [==============================] - 19s 383ms/step - loss: 2.5940 - val_loss: 3.1857\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5711\n",
      "Epoch 1: val_loss did not improve from 2.78314\n",
      "49/49 [==============================] - 19s 399ms/step - loss: 2.5711 - val_loss: 3.0267\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5819\n",
      "Epoch 1: val_loss improved from 2.78314 to 2.75846, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 21s 425ms/step - loss: 2.5819 - val_loss: 2.7585\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6662\n",
      "Epoch 1: val_loss did not improve from 2.75846\n",
      "49/49 [==============================] - 20s 417ms/step - loss: 2.6662 - val_loss: 2.9476\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9112\n",
      "Epoch 1: val_loss did not improve from 2.75846\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.9112 - val_loss: 2.9336\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5665\n",
      "Epoch 1: val_loss did not improve from 2.75846\n",
      "49/49 [==============================] - 21s 432ms/step - loss: 2.5665 - val_loss: 3.1669\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5506\n",
      "Epoch 1: val_loss did not improve from 2.75846\n",
      "49/49 [==============================] - 21s 432ms/step - loss: 2.5506 - val_loss: 3.0233\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5631\n",
      "Epoch 1: val_loss improved from 2.75846 to 2.72842, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 446ms/step - loss: 2.5631 - val_loss: 2.7284\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6372\n",
      "Epoch 1: val_loss did not improve from 2.72842\n",
      "49/49 [==============================] - 21s 431ms/step - loss: 2.6372 - val_loss: 2.9012\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8611\n",
      "Epoch 1: val_loss did not improve from 2.72842\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.8611 - val_loss: 2.9028\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5437\n",
      "Epoch 1: val_loss did not improve from 2.72842\n",
      "49/49 [==============================] - 21s 432ms/step - loss: 2.5437 - val_loss: 3.1334\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5334\n",
      "Epoch 1: val_loss did not improve from 2.72842\n",
      "49/49 [==============================] - 21s 427ms/step - loss: 2.5334 - val_loss: 3.0072\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5432\n",
      "Epoch 1: val_loss improved from 2.72842 to 2.70746, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 21s 437ms/step - loss: 2.5432 - val_loss: 2.7075\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6202\n",
      "Epoch 1: val_loss did not improve from 2.70746\n",
      "49/49 [==============================] - 20s 422ms/step - loss: 2.6202 - val_loss: 2.8647\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8483\n",
      "Epoch 1: val_loss did not improve from 2.70746\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.8483 - val_loss: 2.8910\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5355\n",
      "Epoch 1: val_loss did not improve from 2.70746\n",
      "49/49 [==============================] - 21s 431ms/step - loss: 2.5355 - val_loss: 3.1067\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5163\n",
      "Epoch 1: val_loss did not improve from 2.70746\n",
      "49/49 [==============================] - 21s 428ms/step - loss: 2.5163 - val_loss: 2.9581\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5315\n",
      "Epoch 1: val_loss improved from 2.70746 to 2.68654, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 21s 438ms/step - loss: 2.5315 - val_loss: 2.6865\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6046\n",
      "Epoch 1: val_loss did not improve from 2.68654\n",
      "49/49 [==============================] - 21s 426ms/step - loss: 2.6046 - val_loss: 2.8460\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8335\n",
      "Epoch 1: val_loss did not improve from 2.68654\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.8335 - val_loss: 2.8508\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5327\n",
      "Epoch 1: val_loss did not improve from 2.68654\n",
      "49/49 [==============================] - 21s 441ms/step - loss: 2.5327 - val_loss: 3.0942\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5067\n",
      "Epoch 1: val_loss did not improve from 2.68654\n",
      "49/49 [==============================] - 21s 435ms/step - loss: 2.5067 - val_loss: 2.9386\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5036\n",
      "Epoch 1: val_loss improved from 2.68654 to 2.67560, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 451ms/step - loss: 2.5036 - val_loss: 2.6756\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5705\n",
      "Epoch 1: val_loss did not improve from 2.67560\n",
      "49/49 [==============================] - 21s 435ms/step - loss: 2.5705 - val_loss: 2.8692\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7971\n",
      "Epoch 1: val_loss did not improve from 2.67560\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.7971 - val_loss: 2.8719\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4993\n",
      "Epoch 1: val_loss did not improve from 2.67560\n",
      "49/49 [==============================] - 21s 442ms/step - loss: 2.4993 - val_loss: 3.1292\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4692\n",
      "Epoch 1: val_loss did not improve from 2.67560\n",
      "49/49 [==============================] - 21s 441ms/step - loss: 2.4692 - val_loss: 2.9403\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4773\n",
      "Epoch 1: val_loss improved from 2.67560 to 2.65349, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 452ms/step - loss: 2.4773 - val_loss: 2.6535\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5362\n",
      "Epoch 1: val_loss did not improve from 2.65349\n",
      "49/49 [==============================] - 21s 434ms/step - loss: 2.5362 - val_loss: 2.8281\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7511\n",
      "Epoch 1: val_loss did not improve from 2.65349\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.7511 - val_loss: 2.8356\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4663\n",
      "Epoch 1: val_loss did not improve from 2.65349\n",
      "49/49 [==============================] - 21s 440ms/step - loss: 2.4663 - val_loss: 3.1025\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4440\n",
      "Epoch 1: val_loss did not improve from 2.65349\n",
      "49/49 [==============================] - 21s 440ms/step - loss: 2.4440 - val_loss: 2.8876\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4526\n",
      "Epoch 1: val_loss improved from 2.65349 to 2.62590, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 461ms/step - loss: 2.4526 - val_loss: 2.6259\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5113\n",
      "Epoch 1: val_loss did not improve from 2.62590\n",
      "49/49 [==============================] - 23s 472ms/step - loss: 2.5113 - val_loss: 2.8068\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7239\n",
      "Epoch 1: val_loss did not improve from 2.62590\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.7239 - val_loss: 2.8257\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4443\n",
      "Epoch 1: val_loss did not improve from 2.62590\n",
      "49/49 [==============================] - 22s 453ms/step - loss: 2.4443 - val_loss: 3.0704\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4206\n",
      "Epoch 1: val_loss did not improve from 2.62590\n",
      "49/49 [==============================] - 22s 451ms/step - loss: 2.4206 - val_loss: 2.8721\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4249\n",
      "Epoch 1: val_loss improved from 2.62590 to 2.60701, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 460ms/step - loss: 2.4249 - val_loss: 2.6070\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4848\n",
      "Epoch 1: val_loss did not improve from 2.60701\n",
      "49/49 [==============================] - 22s 450ms/step - loss: 2.4848 - val_loss: 2.8187\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7263\n",
      "Epoch 1: val_loss did not improve from 2.60701\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.7263 - val_loss: 2.7895\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4289\n",
      "Epoch 1: val_loss did not improve from 2.60701\n",
      "49/49 [==============================] - 23s 479ms/step - loss: 2.4289 - val_loss: 3.0631\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4169\n",
      "Epoch 1: val_loss did not improve from 2.60701\n",
      "49/49 [==============================] - 24s 488ms/step - loss: 2.4169 - val_loss: 2.8557\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4237\n",
      "Epoch 1: val_loss improved from 2.60701 to 2.59660, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 24s 487ms/step - loss: 2.4237 - val_loss: 2.5966\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4690\n",
      "Epoch 1: val_loss did not improve from 2.59660\n",
      "49/49 [==============================] - 22s 456ms/step - loss: 2.4690 - val_loss: 2.7768\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6717\n",
      "Epoch 1: val_loss did not improve from 2.59660\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.6717 - val_loss: 2.7802\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4047\n",
      "Epoch 1: val_loss did not improve from 2.59660\n",
      "49/49 [==============================] - 22s 453ms/step - loss: 2.4047 - val_loss: 3.0606\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3807\n",
      "Epoch 1: val_loss did not improve from 2.59660\n",
      "49/49 [==============================] - 21s 439ms/step - loss: 2.3807 - val_loss: 2.8396\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3858\n",
      "Epoch 1: val_loss improved from 2.59660 to 2.58436, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 22s 461ms/step - loss: 2.3858 - val_loss: 2.5844\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4429\n",
      "Epoch 1: val_loss did not improve from 2.58436\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 2.4429 - val_loss: 2.7687\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6610\n",
      "Epoch 1: val_loss did not improve from 2.58436\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.6610 - val_loss: 2.7742\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3748\n",
      "Epoch 1: val_loss did not improve from 2.58436\n",
      "49/49 [==============================] - 24s 493ms/step - loss: 2.3748 - val_loss: 3.0367\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3606\n",
      "Epoch 1: val_loss did not improve from 2.58436\n",
      "49/49 [==============================] - 24s 495ms/step - loss: 2.3606 - val_loss: 2.8623\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3611\n",
      "Epoch 1: val_loss improved from 2.58436 to 2.55687, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 25s 509ms/step - loss: 2.3611 - val_loss: 2.5569\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4167\n",
      "Epoch 1: val_loss did not improve from 2.55687\n",
      "49/49 [==============================] - 25s 505ms/step - loss: 2.4167 - val_loss: 2.7314\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6239\n",
      "Epoch 1: val_loss did not improve from 2.55687\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.6239 - val_loss: 2.7390\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3546\n",
      "Epoch 1: val_loss did not improve from 2.55687\n",
      "49/49 [==============================] - 25s 514ms/step - loss: 2.3546 - val_loss: 2.9811\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3321\n",
      "Epoch 1: val_loss did not improve from 2.55687\n",
      "49/49 [==============================] - 25s 517ms/step - loss: 2.3321 - val_loss: 2.8098\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3369\n",
      "Epoch 1: val_loss improved from 2.55687 to 2.53215, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 25s 523ms/step - loss: 2.3369 - val_loss: 2.5321\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3916\n",
      "Epoch 1: val_loss did not improve from 2.53215\n",
      "49/49 [==============================] - 25s 508ms/step - loss: 2.3916 - val_loss: 2.7135\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5970\n",
      "Epoch 1: val_loss did not improve from 2.53215\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5970 - val_loss: 2.7329\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3341\n",
      "Epoch 1: val_loss did not improve from 2.53215\n",
      "49/49 [==============================] - 25s 517ms/step - loss: 2.3341 - val_loss: 2.9978\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3288\n",
      "Epoch 1: val_loss did not improve from 2.53215\n",
      "49/49 [==============================] - 25s 516ms/step - loss: 2.3288 - val_loss: 2.7501\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3295\n",
      "Epoch 1: val_loss improved from 2.53215 to 2.51387, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 528ms/step - loss: 2.3295 - val_loss: 2.5139\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3943\n",
      "Epoch 1: val_loss did not improve from 2.51387\n",
      "49/49 [==============================] - 25s 513ms/step - loss: 2.3943 - val_loss: 2.6982\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5913\n",
      "Epoch 1: val_loss did not improve from 2.51387\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5913 - val_loss: 2.7045\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3211\n",
      "Epoch 1: val_loss did not improve from 2.51387\n",
      "49/49 [==============================] - 25s 520ms/step - loss: 2.3211 - val_loss: 3.0015\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2942\n",
      "Epoch 1: val_loss did not improve from 2.51387\n",
      "49/49 [==============================] - 25s 522ms/step - loss: 2.2942 - val_loss: 2.7893\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3107\n",
      "Epoch 1: val_loss improved from 2.51387 to 2.49780, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 535ms/step - loss: 2.3107 - val_loss: 2.4978\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3563\n",
      "Epoch 1: val_loss did not improve from 2.49780\n",
      "49/49 [==============================] - 26s 533ms/step - loss: 2.3563 - val_loss: 2.6984\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5713\n",
      "Epoch 1: val_loss did not improve from 2.49780\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5713 - val_loss: 2.6982\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2961\n",
      "Epoch 1: val_loss did not improve from 2.49780\n",
      "49/49 [==============================] - 26s 534ms/step - loss: 2.2961 - val_loss: 2.9816\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2753\n",
      "Epoch 1: val_loss did not improve from 2.49780\n",
      "49/49 [==============================] - 26s 529ms/step - loss: 2.2753 - val_loss: 2.7375\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2869\n",
      "Epoch 1: val_loss improved from 2.49780 to 2.48173, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 541ms/step - loss: 2.2869 - val_loss: 2.4817\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3315\n",
      "Epoch 1: val_loss did not improve from 2.48173\n",
      "49/49 [==============================] - 26s 529ms/step - loss: 2.3315 - val_loss: 2.6625\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5407\n",
      "Epoch 1: val_loss did not improve from 2.48173\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5407 - val_loss: 2.6728\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2772\n",
      "Epoch 1: val_loss did not improve from 2.48173\n",
      "49/49 [==============================] - 26s 530ms/step - loss: 2.2772 - val_loss: 2.9506\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2485\n",
      "Epoch 1: val_loss did not improve from 2.48173\n",
      "49/49 [==============================] - 25s 521ms/step - loss: 2.2485 - val_loss: 2.7282\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2626\n",
      "Epoch 1: val_loss improved from 2.48173 to 2.47397, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 528ms/step - loss: 2.2626 - val_loss: 2.4740\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3148\n",
      "Epoch 1: val_loss did not improve from 2.47397\n",
      "49/49 [==============================] - 24s 505ms/step - loss: 2.3148 - val_loss: 2.6691\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5243\n",
      "Epoch 1: val_loss did not improve from 2.47397\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5243 - val_loss: 2.6458\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2573\n",
      "Epoch 1: val_loss did not improve from 2.47397\n",
      "49/49 [==============================] - 25s 506ms/step - loss: 2.2573 - val_loss: 2.9490\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2342\n",
      "Epoch 1: val_loss did not improve from 2.47397\n",
      "49/49 [==============================] - 24s 501ms/step - loss: 2.2342 - val_loss: 2.7314\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2410\n",
      "Epoch 1: val_loss improved from 2.47397 to 2.45027, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 25s 507ms/step - loss: 2.2410 - val_loss: 2.4503\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2989\n",
      "Epoch 1: val_loss did not improve from 2.45027\n",
      "49/49 [==============================] - 24s 493ms/step - loss: 2.2989 - val_loss: 2.6690\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5051\n",
      "Epoch 1: val_loss did not improve from 2.45027\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.5051 - val_loss: 2.6718\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2461\n",
      "Epoch 1: val_loss did not improve from 2.45027\n",
      "49/49 [==============================] - 24s 499ms/step - loss: 2.2461 - val_loss: 2.9620\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2216\n",
      "Epoch 1: val_loss did not improve from 2.45027\n",
      "49/49 [==============================] - 24s 497ms/step - loss: 2.2216 - val_loss: 2.7350\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2188\n",
      "Epoch 1: val_loss improved from 2.45027 to 2.43003, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 25s 513ms/step - loss: 2.2188 - val_loss: 2.4300\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2745\n",
      "Epoch 1: val_loss did not improve from 2.43003\n",
      "49/49 [==============================] - 24s 499ms/step - loss: 2.2745 - val_loss: 2.6207\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4841\n",
      "Epoch 1: val_loss did not improve from 2.43003\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4841 - val_loss: 2.6211\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2185\n",
      "Epoch 1: val_loss did not improve from 2.43003\n",
      "49/49 [==============================] - 25s 508ms/step - loss: 2.2185 - val_loss: 2.8992\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1954\n",
      "Epoch 1: val_loss did not improve from 2.43003\n",
      "49/49 [==============================] - 25s 510ms/step - loss: 2.1954 - val_loss: 2.6764\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1989\n",
      "Epoch 1: val_loss improved from 2.43003 to 2.40539, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 530ms/step - loss: 2.1989 - val_loss: 2.4054\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2461\n",
      "Epoch 1: val_loss did not improve from 2.40539\n",
      "49/49 [==============================] - 25s 517ms/step - loss: 2.2461 - val_loss: 2.6150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 1: val_loss did not improve from 2.40539\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4519 - val_loss: 2.6176\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1909\n",
      "Epoch 1: val_loss did not improve from 2.40539\n",
      "49/49 [==============================] - 26s 527ms/step - loss: 2.1909 - val_loss: 2.8696\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1721\n",
      "Epoch 1: val_loss did not improve from 2.40539\n",
      "49/49 [==============================] - 26s 528ms/step - loss: 2.1721 - val_loss: 2.7563\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1773\n",
      "Epoch 1: val_loss improved from 2.40539 to 2.38522, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 540ms/step - loss: 2.1773 - val_loss: 2.3852\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2251\n",
      "Epoch 1: val_loss did not improve from 2.38522\n",
      "49/49 [==============================] - 26s 529ms/step - loss: 2.2251 - val_loss: 2.5810\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 1: val_loss did not improve from 2.38522\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4364 - val_loss: 2.5864\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1757\n",
      "Epoch 1: val_loss did not improve from 2.38522\n",
      "49/49 [==============================] - 26s 531ms/step - loss: 2.1757 - val_loss: 2.8880\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1633\n",
      "Epoch 1: val_loss did not improve from 2.38522\n",
      "49/49 [==============================] - 26s 525ms/step - loss: 2.1633 - val_loss: 2.6820\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1663\n",
      "Epoch 1: val_loss improved from 2.38522 to 2.37509, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 535ms/step - loss: 2.1663 - val_loss: 2.3751\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2093\n",
      "Epoch 1: val_loss did not improve from 2.37509\n",
      "49/49 [==============================] - 25s 519ms/step - loss: 2.2093 - val_loss: 2.5987\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 1: val_loss did not improve from 2.37509\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4261 - val_loss: 2.5925\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1577\n",
      "Epoch 1: val_loss did not improve from 2.37509\n",
      "49/49 [==============================] - 26s 525ms/step - loss: 2.1577 - val_loss: 2.8657\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1346\n",
      "Epoch 1: val_loss did not improve from 2.37509\n",
      "49/49 [==============================] - 26s 528ms/step - loss: 2.1346 - val_loss: 2.6203\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1411\n",
      "Epoch 1: val_loss improved from 2.37509 to 2.34860, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 555ms/step - loss: 2.1411 - val_loss: 2.3486\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1855\n",
      "Epoch 1: val_loss did not improve from 2.34860\n",
      "49/49 [==============================] - 26s 541ms/step - loss: 2.1855 - val_loss: 2.5749\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4083\n",
      "Epoch 1: val_loss did not improve from 2.34860\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.4083 - val_loss: 2.5699\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1338\n",
      "Epoch 1: val_loss did not improve from 2.34860\n",
      "49/49 [==============================] - 27s 549ms/step - loss: 2.1338 - val_loss: 2.8587\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1089\n",
      "Epoch 1: val_loss did not improve from 2.34860\n",
      "49/49 [==============================] - 27s 549ms/step - loss: 2.1089 - val_loss: 2.5785\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1160\n",
      "Epoch 1: val_loss improved from 2.34860 to 2.33436, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 557ms/step - loss: 2.1160 - val_loss: 2.3344\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1567\n",
      "Epoch 1: val_loss did not improve from 2.33436\n",
      "49/49 [==============================] - 26s 537ms/step - loss: 2.1567 - val_loss: 2.5836\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3795\n",
      "Epoch 1: val_loss did not improve from 2.33436\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.3795 - val_loss: 2.5736\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1070\n",
      "Epoch 1: val_loss did not improve from 2.33436\n",
      "49/49 [==============================] - 26s 537ms/step - loss: 2.1070 - val_loss: 2.8508\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0857\n",
      "Epoch 1: val_loss did not improve from 2.33436\n",
      "49/49 [==============================] - 26s 536ms/step - loss: 2.0857 - val_loss: 2.6493\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0960\n",
      "Epoch 1: val_loss improved from 2.33436 to 2.31759, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 554ms/step - loss: 2.0960 - val_loss: 2.3176\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1436\n",
      "Epoch 1: val_loss did not improve from 2.31759\n",
      "49/49 [==============================] - 26s 534ms/step - loss: 2.1436 - val_loss: 2.5206\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3550\n",
      "Epoch 1: val_loss did not improve from 2.31759\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.3550 - val_loss: 2.5470\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0914\n",
      "Epoch 1: val_loss did not improve from 2.31759\n",
      "49/49 [==============================] - 26s 541ms/step - loss: 2.0914 - val_loss: 2.8459\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0810\n",
      "Epoch 1: val_loss did not improve from 2.31759\n",
      "49/49 [==============================] - 26s 540ms/step - loss: 2.0810 - val_loss: 2.6206\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0843\n",
      "Epoch 1: val_loss improved from 2.31759 to 2.30419, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 553ms/step - loss: 2.0843 - val_loss: 2.3042\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1222\n",
      "Epoch 1: val_loss did not improve from 2.30419\n",
      "49/49 [==============================] - 26s 537ms/step - loss: 2.1222 - val_loss: 2.5288\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3533\n",
      "Epoch 1: val_loss did not improve from 2.30419\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.3533 - val_loss: 2.5203\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0824\n",
      "Epoch 1: val_loss did not improve from 2.30419\n",
      "49/49 [==============================] - 26s 541ms/step - loss: 2.0824 - val_loss: 2.8359\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0604\n",
      "Epoch 1: val_loss did not improve from 2.30419\n",
      "49/49 [==============================] - 26s 541ms/step - loss: 2.0604 - val_loss: 2.6016\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0610\n",
      "Epoch 1: val_loss improved from 2.30419 to 2.28901, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 28s 568ms/step - loss: 2.0610 - val_loss: 2.2890\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1080\n",
      "Epoch 1: val_loss did not improve from 2.28901\n",
      "49/49 [==============================] - 26s 535ms/step - loss: 2.1080 - val_loss: 2.5418\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3431\n",
      "Epoch 1: val_loss did not improve from 2.28901\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.3431 - val_loss: 2.5308\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0473\n",
      "Epoch 1: val_loss did not improve from 2.28901\n",
      "49/49 [==============================] - 26s 535ms/step - loss: 2.0473 - val_loss: 2.7888\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0270\n",
      "Epoch 1: val_loss did not improve from 2.28901\n",
      "49/49 [==============================] - 26s 527ms/step - loss: 2.0270 - val_loss: 2.6490\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0357\n",
      "Epoch 1: val_loss improved from 2.28901 to 2.27536, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 26s 543ms/step - loss: 2.0357 - val_loss: 2.2754\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0732\n",
      "Epoch 1: val_loss did not improve from 2.27536\n",
      "49/49 [==============================] - 25s 525ms/step - loss: 2.0732 - val_loss: 2.5228\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2901\n",
      "Epoch 1: val_loss did not improve from 2.27536\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2901 - val_loss: 2.5123\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0257\n",
      "Epoch 1: val_loss did not improve from 2.27536\n",
      "49/49 [==============================] - 26s 528ms/step - loss: 2.0257 - val_loss: 2.8030\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0066\n",
      "Epoch 1: val_loss did not improve from 2.27536\n",
      "49/49 [==============================] - 26s 529ms/step - loss: 2.0066 - val_loss: 2.5382\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0153\n",
      "Epoch 1: val_loss improved from 2.27536 to 2.25046, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 558ms/step - loss: 2.0153 - val_loss: 2.2505\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0573\n",
      "Epoch 1: val_loss did not improve from 2.25046\n",
      "49/49 [==============================] - 26s 539ms/step - loss: 2.0573 - val_loss: 2.4790\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2760\n",
      "Epoch 1: val_loss did not improve from 2.25046\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2760 - val_loss: 2.5051\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0145\n",
      "Epoch 1: val_loss did not improve from 2.25046\n",
      "49/49 [==============================] - 27s 549ms/step - loss: 2.0145 - val_loss: 2.7957\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0021\n",
      "Epoch 1: val_loss did not improve from 2.25046\n",
      "49/49 [==============================] - 27s 552ms/step - loss: 2.0021 - val_loss: 2.5506\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0122\n",
      "Epoch 1: val_loss improved from 2.25046 to 2.23971, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 28s 573ms/step - loss: 2.0122 - val_loss: 2.2397\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0421\n",
      "Epoch 1: val_loss did not improve from 2.23971\n",
      "49/49 [==============================] - 27s 552ms/step - loss: 2.0421 - val_loss: 2.4768\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2759\n",
      "Epoch 1: val_loss did not improve from 2.23971\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2759 - val_loss: 2.4778\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9966\n",
      "Epoch 1: val_loss did not improve from 2.23971\n",
      "49/49 [==============================] - 27s 551ms/step - loss: 1.9966 - val_loss: 2.7941\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9790\n",
      "Epoch 1: val_loss did not improve from 2.23971\n",
      "49/49 [==============================] - 27s 551ms/step - loss: 1.9790 - val_loss: 2.5130\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9796\n",
      "Epoch 1: val_loss improved from 2.23971 to 2.23300, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 562ms/step - loss: 1.9796 - val_loss: 2.2330\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0211\n",
      "Epoch 1: val_loss did not improve from 2.23300\n",
      "49/49 [==============================] - 26s 544ms/step - loss: 2.0211 - val_loss: 2.4298\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2650\n",
      "Epoch 1: val_loss did not improve from 2.23300\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2650 - val_loss: 2.4519\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9743\n",
      "Epoch 1: val_loss did not improve from 2.23300\n",
      "49/49 [==============================] - 27s 561ms/step - loss: 1.9743 - val_loss: 2.7536\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9496\n",
      "Epoch 1: val_loss did not improve from 2.23300\n",
      "49/49 [==============================] - 27s 549ms/step - loss: 1.9496 - val_loss: 2.4886\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9488\n",
      "Epoch 1: val_loss improved from 2.23300 to 2.20481, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 27s 566ms/step - loss: 1.9488 - val_loss: 2.2048\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9932\n",
      "Epoch 1: val_loss did not improve from 2.20481\n",
      "49/49 [==============================] - 27s 553ms/step - loss: 1.9932 - val_loss: 2.4537\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2356\n",
      "Epoch 1: val_loss did not improve from 2.20481\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2356 - val_loss: 2.4612\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9506\n",
      "Epoch 1: val_loss did not improve from 2.20481\n",
      "49/49 [==============================] - 28s 583ms/step - loss: 1.9506 - val_loss: 2.7333\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9382\n",
      "Epoch 1: val_loss did not improve from 2.20481\n",
      "49/49 [==============================] - 29s 596ms/step - loss: 1.9382 - val_loss: 2.5177\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9421\n",
      "Epoch 1: val_loss improved from 2.20481 to 2.18575, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 30s 616ms/step - loss: 1.9421 - val_loss: 2.1857\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9937\n",
      "Epoch 1: val_loss did not improve from 2.18575\n",
      "49/49 [==============================] - 29s 591ms/step - loss: 1.9937 - val_loss: 2.4714\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3713\n",
      "Epoch 1: val_loss did not improve from 2.18575\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.3713 - val_loss: 2.4353\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9596\n",
      "Epoch 1: val_loss did not improve from 2.18575\n",
      "49/49 [==============================] - 29s 588ms/step - loss: 1.9596 - val_loss: 2.7186\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9261\n",
      "Epoch 1: val_loss did not improve from 2.18575\n",
      "49/49 [==============================] - 29s 596ms/step - loss: 1.9261 - val_loss: 2.4443\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9347\n",
      "Epoch 1: val_loss improved from 2.18575 to 2.16367, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 28s 585ms/step - loss: 1.9347 - val_loss: 2.1637\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9818\n",
      "Epoch 1: val_loss did not improve from 2.16367\n",
      "49/49 [==============================] - 28s 570ms/step - loss: 1.9818 - val_loss: 2.4492\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2043\n",
      "Epoch 1: val_loss did not improve from 2.16367\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.2043 - val_loss: 2.4581\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9205\n",
      "Epoch 1: val_loss did not improve from 2.16367\n",
      "49/49 [==============================] - 28s 569ms/step - loss: 1.9205 - val_loss: 2.7406\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9012\n",
      "Epoch 1: val_loss did not improve from 2.16367\n",
      "49/49 [==============================] - 28s 566ms/step - loss: 1.9012 - val_loss: 2.4750\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9087\n",
      "Epoch 1: val_loss improved from 2.16367 to 2.15964, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 28s 577ms/step - loss: 1.9087 - val_loss: 2.1596\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9435\n",
      "Epoch 1: val_loss did not improve from 2.15964\n",
      "49/49 [==============================] - 27s 565ms/step - loss: 1.9435 - val_loss: 2.4067\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1904\n",
      "Epoch 1: val_loss did not improve from 2.15964\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1904 - val_loss: 2.4196\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9013\n",
      "Epoch 1: val_loss did not improve from 2.15964\n",
      "49/49 [==============================] - 28s 575ms/step - loss: 1.9013 - val_loss: 2.7111\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8787\n",
      "Epoch 1: val_loss did not improve from 2.15964\n",
      "49/49 [==============================] - 28s 583ms/step - loss: 1.8787 - val_loss: 2.4198\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8782\n",
      "Epoch 1: val_loss improved from 2.15964 to 2.13215, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 29s 605ms/step - loss: 1.8782 - val_loss: 2.1321\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9148\n",
      "Epoch 1: val_loss did not improve from 2.13215\n",
      "49/49 [==============================] - 28s 575ms/step - loss: 1.9148 - val_loss: 2.3881\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1644\n",
      "Epoch 1: val_loss did not improve from 2.13215\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1644 - val_loss: 2.3842\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8814\n",
      "Epoch 1: val_loss did not improve from 2.13215\n",
      "49/49 [==============================] - 28s 586ms/step - loss: 1.8814 - val_loss: 2.7179\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8596\n",
      "Epoch 1: val_loss did not improve from 2.13215\n",
      "49/49 [==============================] - 28s 585ms/step - loss: 1.8596 - val_loss: 2.4948\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8579\n",
      "Epoch 1: val_loss improved from 2.13215 to 2.11851, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 29s 589ms/step - loss: 1.8579 - val_loss: 2.1185\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8930\n",
      "Epoch 1: val_loss did not improve from 2.11851\n",
      "49/49 [==============================] - 27s 566ms/step - loss: 1.8930 - val_loss: 2.3895\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1420\n",
      "Epoch 1: val_loss did not improve from 2.11851\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1420 - val_loss: 2.3926\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8545\n",
      "Epoch 1: val_loss did not improve from 2.11851\n",
      "49/49 [==============================] - 28s 575ms/step - loss: 1.8545 - val_loss: 2.6978\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8424\n",
      "Epoch 1: val_loss did not improve from 2.11851\n",
      "49/49 [==============================] - 28s 579ms/step - loss: 1.8424 - val_loss: 2.3820\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8566\n",
      "Epoch 1: val_loss improved from 2.11851 to 2.11378, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 29s 590ms/step - loss: 1.8566 - val_loss: 2.1138\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8959\n",
      "Epoch 1: val_loss did not improve from 2.11378\n",
      "49/49 [==============================] - 28s 572ms/step - loss: 1.8959 - val_loss: 2.3681\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1473\n",
      "Epoch 1: val_loss did not improve from 2.11378\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1473 - val_loss: 2.3682\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8453\n",
      "Epoch 1: val_loss did not improve from 2.11378\n",
      "49/49 [==============================] - 28s 579ms/step - loss: 1.8453 - val_loss: 2.7348\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8272\n",
      "Epoch 1: val_loss did not improve from 2.11378\n",
      "49/49 [==============================] - 28s 579ms/step - loss: 1.8272 - val_loss: 2.3919\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8252\n",
      "Epoch 1: val_loss improved from 2.11378 to 2.09229, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 29s 597ms/step - loss: 1.8252 - val_loss: 2.0923\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8659\n",
      "Epoch 1: val_loss did not improve from 2.09229\n",
      "49/49 [==============================] - 28s 583ms/step - loss: 1.8659 - val_loss: 2.3634\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1299\n",
      "Epoch 1: val_loss did not improve from 2.09229\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1299 - val_loss: 2.3701\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8197\n",
      "Epoch 1: val_loss did not improve from 2.09229\n",
      "49/49 [==============================] - 29s 594ms/step - loss: 1.8197 - val_loss: 2.6735\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8074\n",
      "Epoch 1: val_loss did not improve from 2.09229\n",
      "49/49 [==============================] - 29s 601ms/step - loss: 1.8074 - val_loss: 2.4431\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8104\n",
      "Epoch 1: val_loss improved from 2.09229 to 2.07581, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 30s 614ms/step - loss: 1.8104 - val_loss: 2.0758\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8418\n",
      "Epoch 1: val_loss did not improve from 2.07581\n",
      "49/49 [==============================] - 29s 602ms/step - loss: 1.8418 - val_loss: 2.3651\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1151\n",
      "Epoch 1: val_loss did not improve from 2.07581\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1151 - val_loss: 2.3517\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7951\n",
      "Epoch 1: val_loss did not improve from 2.07581\n",
      "49/49 [==============================] - 29s 601ms/step - loss: 1.7951 - val_loss: 2.6723\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7840\n",
      "Epoch 1: val_loss did not improve from 2.07581\n",
      "49/49 [==============================] - 29s 603ms/step - loss: 1.7840 - val_loss: 2.4381\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7937\n",
      "Epoch 1: val_loss improved from 2.07581 to 2.04971, saving model to trained_events_model_second_batch_validation_1/cp.ckpt\n",
      "49/49 [==============================] - 30s 608ms/step - loss: 1.7937 - val_loss: 2.0497\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8297\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 594ms/step - loss: 1.8297 - val_loss: 2.3476\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1051\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.1051 - val_loss: 2.3234\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7790\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 603ms/step - loss: 1.7790 - val_loss: 2.6553\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7701\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 598ms/step - loss: 1.7701 - val_loss: 2.3633\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7718\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 30s 609ms/step - loss: 1.7718 - val_loss: 2.0501\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8183\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 600ms/step - loss: 1.8183 - val_loss: 2.2898\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1000\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1000 - val_loss: 2.2954\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7884\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 604ms/step - loss: 1.7884 - val_loss: 2.5775\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7574\n",
      "Epoch 1: val_loss did not improve from 2.04971\n",
      "49/49 [==============================] - 29s 600ms/step - loss: 1.7574 - val_loss: 2.3674\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_dataset(x, y)\n\u001b[1;32m     21\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m histories[\u001b[38;5;241m0\u001b[39m][epoch] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory\n\u001b[1;32m     27\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1408\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1409\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1419\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py:1710\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1709\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 1710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1712\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/data_adapter.py:1191\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1191\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    485\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:755\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 755\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    784\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v2(\n\u001b[1;32m    785\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    786\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;66;03m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[1;32m    789\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_deleter \u001b[38;5;241m=\u001b[39m IteratorResourceDeleter(\n\u001b[1;32m    790\u001b[0m       handle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[1;32m    791\u001b[0m       deleter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3315\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3314\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3315\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3318\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 12500\n",
    "end_count = len(input_sequences)\n",
    "histories = [{}]\n",
    "\n",
    "EPOCHS = 60\n",
    "epoch = 1 \n",
    "\n",
    "while epoch <= EPOCHS:\n",
    "    curr_start = 0\n",
    "    curr_end = 0\n",
    "    \n",
    "    while curr_end < end_count:\n",
    "    \n",
    "        curr_start = curr_end\n",
    "        curr_end = min(curr_start + batch_size, end_count)\n",
    "\n",
    "        x, y = get_features_and_labels(input_sequences[curr_start:curr_end])\n",
    "\n",
    "        dataset = get_dataset(x, y)\n",
    "        \n",
    "        x, y = None, None\n",
    "        \n",
    "        model.fit(dataset, validation_data=test_dataset, epochs=1, callbacks=[cp_callback])\n",
    "        \n",
    "    histories[0][epoch] = model.history.history\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d31dada-91c8-4a60-87f5-da937f7b1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickle/secondbatchloss1', 'wb') as f:\n",
    "    pickle.dump(model.history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c4b5414-b92c-43f3-9559-3a737daf8352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_events_model_second_batch_validation_1/cp.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d4907c5-18b6-4cdb-af01-13cb105abf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_sequences/events_test.npy', 'rb') as f:\n",
    "    fsz = os.fstat(f.fileno()).st_size\n",
    "    input_test_sequences = np.load(f)\n",
    "    while f.tell() < fsz:\n",
    "        input_test_sequences = np.vstack((input_sequences, np.load(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d5e9938-e2ae-4493-a76c-433d9eefe516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2445ed2-5c40-450c-9f44-d0d6210f21e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 12s 53ms/step - loss: 2.0497\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = get_features_and_labels(input_test_sequences)\n",
    "\n",
    "test_dataset = get_dataset(x_test, y_test)\n",
    "\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6371d59b-0482-4519-a3e5-a042b6417fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 19), dtype=tf.int32, name=None), TensorSpec(shape=(None, 10878), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = None\n",
    "x_test, y_test = None, None\n",
    "input_test_sequences = None\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b897369b-2aca-421d-854a-2627588d40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a871f8ed-271a-449f-8092-3db19759ecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7ec6307-35db-4bef-ac82-277eec0cff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, words_from_ids, ids_from_words, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.words_from_ids = words_from_ids\n",
    "        self.ids_from_words = ids_from_words\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_words.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "    \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "    \n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, state1, state2 = self.model(inputs=inputs, states=states,\n",
    "                                              return_state=True)\n",
    "    \n",
    "        # Return the characters and model state.\n",
    "        return predicted_logits, [state1, state2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "760b20b7-3eed-433f-851f-9d83d278dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, words_from_ids, ids_from_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bdfae96-fb12-4e8d-84b6-4845fb581ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = open('test_events.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "corpus = test_data.lower().split(\"\\n\")\n",
    "\n",
    "story_text = []\n",
    "\n",
    "events_elapsed = []\n",
    "events_actual_next = []\n",
    "\n",
    "\n",
    "for line in corpus:\n",
    "    if line == 'story start':\n",
    "        story_text = []\n",
    "        continue\n",
    "        \n",
    "    if line == 'story end':\n",
    "        actual = list(filter(None, story_text[-1].split(' ')))\n",
    "        events_actual_next.append(actual)\n",
    "    \n",
    "        events_elapsed.append([list(filter(None, ''.join(story_text[:-1]).split(' ')))])\n",
    "        \n",
    "        # print(words_from_ids(elapsed_tensor))\n",
    "        # print(actual)\n",
    "        \n",
    "        story_text = []\n",
    "        continue\n",
    "    \n",
    "    story_text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3912bc9-6f74-4735-a0ad-e5dfb25010d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "[['james', 'business', 'has', 'na', 'he', 'services', 'performs', 'neighborhoods', 'james', 'grass', 'cuts', 'na', 'he', 'branches', 'cuts', 'request']]\n",
      "['james', 'na', 'is', 'happy']\n"
     ]
    }
   ],
   "source": [
    "print(len(events_elapsed))\n",
    "print(events_elapsed[13])\n",
    "print(events_actual_next[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26f8ddc7-75ac-4a6d-aca5-edf6c3cbf562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n",
      "it\n",
      "could\n",
      "na\n",
      "na\n",
      "marks\n",
      "jail\n",
      "dog\n",
      "i\n",
      "na\n",
      "'ve\n",
      "na\n",
      "she\n",
      "it\n",
      "decided\n",
      "na\n",
      "i\n",
      "na\n",
      "had\n",
      "na\n"
     ]
    }
   ],
   "source": [
    "predicted_events = []\n",
    "\n",
    "for event_elapsed in events_elapsed[1000:1005]:\n",
    "    \n",
    "    states = None\n",
    "\n",
    "    seed_words = [[]]\n",
    "    for event in event_elapsed[0]:\n",
    "        seed_words[0].append(event)\n",
    "\n",
    "    event_predicted_next = []\n",
    "\n",
    "    for n in range(4):\n",
    "        seed_w = ids_from_words(seed_words)\n",
    "        \n",
    "        inputs = tf.convert_to_tensor(pad_sequences(seed_w, maxlen=max_sequence_len-1))\n",
    "        \n",
    "        predicted_logits, states = one_step_model.generate_one_step(inputs, states=states)\n",
    "        \n",
    "        predicted_logits = predicted_logits + one_step_model.prediction_mask\n",
    "        \n",
    "        predicted_word = words_from_ids(tf.math.argmax(tf.nn.softmax(predicted_logits[0]), 0))\n",
    "        \n",
    "        predicted_word = predicted_word.numpy().decode()\n",
    "        \n",
    "        print(predicted_word)\n",
    "\n",
    "        event_predicted_next.append(predicted_word)\n",
    "\n",
    "        seed_words[0].append(predicted_word)\n",
    "\n",
    "        seed_words[0] = seed_words[0][-max_sequence_len-1:]\n",
    "    \n",
    "    # predicted_events.append(event_predicted_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e371d98-b019-4c9f-ab6c-35c3633a0448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(len(events_elapsed))\n",
    "print(len(predicted_events))\n",
    "# print(len(events_actual_next))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f383c325-27f7-4cbe-bec2-ee62c576b21c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x394972d30>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 00:17:20.632945: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step_batch_second_validation_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step_batch_second_validation_1/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step_batch_second_validation_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "101a5355-938e-495b-9fdc-bc8d92085c8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 00:17:50.671923: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-12 00:17:50.680176: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-12 00:17:50.694519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-12 00:17:50.698063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "one_step_reloaded = tf.saved_model.load('one_step_batch_second_validation_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f1b0ddd-5346-400c-bc01-a6b4d5d77e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_events = []\n",
    "\n",
    "for event_elapsed in events_elapsed[940:950]:\n",
    "    \n",
    "    states = None\n",
    "\n",
    "    seed_words = [[]]\n",
    "    for event in event_elapsed[0]:\n",
    "        seed_words[0].append(event)\n",
    "\n",
    "    event_predicted_next = []\n",
    "\n",
    "    for n in range(4):\n",
    "        seed_w = ids_from_words(seed_words)\n",
    "        \n",
    "        inputs = tf.convert_to_tensor(pad_sequences(seed_w, maxlen=max_sequence_len-1))\n",
    "        \n",
    "        predicted_logits, states = one_step_reloaded.generate_one_step(inputs, states=states)\n",
    "        \n",
    "        predicted_logits = predicted_logits + one_step_reloaded.prediction_mask\n",
    "        \n",
    "        predicted_word = words_from_ids(tf.math.argmax(tf.nn.softmax(predicted_logits[0]), 0))\n",
    "        \n",
    "        predicted_word = predicted_word.numpy().decode()\n",
    "\n",
    "        event_predicted_next.append(predicted_word)\n",
    "\n",
    "        seed_words[0].append(predicted_word)\n",
    "\n",
    "        seed_words[0] = seed_words[0][-max_sequence_len-1:]\n",
    "    \n",
    "    predicted_events.append(event_predicted_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dd79b58-78d7-488b-b987-03975494496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['counselor', 'na', 'went', 'na'],\n",
       " ['monica', 'na', 'felt', 'guilty'],\n",
       " ['he', 'planes', 'left', 'dog'],\n",
       " ['man', 'him', 'told', 'better'],\n",
       " ['cathy', 'na', 'were', 'great'],\n",
       " ['they', 'na', 'were', 'na'],\n",
       " ['arrow', 'na', 'were', 'able'],\n",
       " ['we', 'na', 'were', 'na'],\n",
       " ['he', 'na', 'is', 'better'],\n",
       " ['i', 'na', 'decided', 'na']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "689a575b-9993-4845-9647-7f2ef1b72523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: {'loss': [2.0643861293792725], 'val_loss': [2.961289405822754]},\n",
       "  2: {'loss': [2.0032708644866943], 'val_loss': [2.942138195037842]},\n",
       "  3: {'loss': [1.9470793008804321], 'val_loss': [2.9699578285217285]},\n",
       "  4: {'loss': [1.911195993423462], 'val_loss': [2.9335718154907227]},\n",
       "  5: {'loss': [1.861051082611084], 'val_loss': [2.90276837348938]},\n",
       "  6: {'loss': [1.8483338356018066], 'val_loss': [2.891018867492676]},\n",
       "  7: {'loss': [1.8335037231445312], 'val_loss': [2.850835084915161]},\n",
       "  8: {'loss': [1.7970795631408691], 'val_loss': [2.871936559677124]},\n",
       "  9: {'loss': [1.7511053085327148], 'val_loss': [2.8355655670166016]},\n",
       "  10: {'loss': [1.723913550376892], 'val_loss': [2.8256797790527344]},\n",
       "  11: {'loss': [1.726291298866272], 'val_loss': [2.789463758468628]},\n",
       "  12: {'loss': [1.6716786623001099], 'val_loss': [2.7801589965820312]},\n",
       "  13: {'loss': [1.661008358001709], 'val_loss': [2.774237871170044]},\n",
       "  14: {'loss': [1.6238725185394287], 'val_loss': [2.738985776901245]},\n",
       "  15: {'loss': [1.5969836711883545], 'val_loss': [2.7328553199768066]},\n",
       "  16: {'loss': [1.5913366079330444], 'val_loss': [2.704486846923828]},\n",
       "  17: {'loss': [1.5712658166885376], 'val_loss': [2.698158025741577]},\n",
       "  18: {'loss': [1.5407044887542725], 'val_loss': [2.6727800369262695]},\n",
       "  19: {'loss': [1.5243417024612427], 'val_loss': [2.6458282470703125]},\n",
       "  20: {'loss': [1.5051064491271973], 'val_loss': [2.6717612743377686]},\n",
       "  21: {'loss': [1.4840760231018066], 'val_loss': [2.621126413345337]},\n",
       "  22: {'loss': [1.4518896341323853], 'val_loss': [2.6176371574401855]},\n",
       "  23: {'loss': [1.43642258644104], 'val_loss': [2.5863893032073975]},\n",
       "  24: {'loss': [1.4260663986206055], 'val_loss': [2.592541217803955]},\n",
       "  25: {'loss': [1.408276081085205], 'val_loss': [2.569894552230835]},\n",
       "  26: {'loss': [1.379468560218811], 'val_loss': [2.573580265045166]},\n",
       "  27: {'loss': [1.3549898862838745], 'val_loss': [2.5469563007354736]},\n",
       "  28: {'loss': [1.353258490562439], 'val_loss': [2.5203325748443604]},\n",
       "  29: {'loss': [1.343088150024414], 'val_loss': [2.530752182006836]},\n",
       "  30: {'loss': [1.2901334762573242], 'val_loss': [2.512275218963623]},\n",
       "  31: {'loss': [1.2759796380996704], 'val_loss': [2.5051381587982178]},\n",
       "  32: {'loss': [1.275931477546692], 'val_loss': [2.477827310562134]},\n",
       "  33: {'loss': [1.2650309801101685], 'val_loss': [2.451927423477173]},\n",
       "  34: {'loss': [1.2355750799179077], 'val_loss': [2.4612038135528564]},\n",
       "  35: {'loss': [1.3712754249572754], 'val_loss': [2.4352822303771973]},\n",
       "  36: {'loss': [1.2043348550796509], 'val_loss': [2.4580745697021484]},\n",
       "  37: {'loss': [1.1904205083847046], 'val_loss': [2.4195754528045654]},\n",
       "  38: {'loss': [1.164367437362671], 'val_loss': [2.3842008113861084]},\n",
       "  39: {'loss': [1.1419992446899414], 'val_loss': [2.3925998210906982]},\n",
       "  40: {'loss': [1.1472716331481934], 'val_loss': [2.368208646774292]},\n",
       "  41: {'loss': [1.1298671960830688], 'val_loss': [2.3700618743896484]},\n",
       "  42: {'loss': [1.1151286363601685], 'val_loss': [2.3517253398895264]},\n",
       "  43: {'loss': [1.1051074266433716], 'val_loss': [2.323413848876953]},\n",
       "  44: {'loss': [1.10002863407135], 'val_loss': [2.295405387878418]}}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f7f35ced-f389-4d0a-891b-f3ccabed962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x2e44e8700>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_reloaded.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc368079-0751-41c6-8730-ce85af9e3067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.losses.CategoricalCrossentropy at 0x16716f280>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_model.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e3850-7a59-490d-b111-2b4804835542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
